{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# libs for text processing\n",
        "from textblob import TextBlob\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "nltk_lemmatizer = WordNetLemmatizer()\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# Stop words libraries\n",
        "from nltk.corpus import stopwords\n",
        "nltk_stop_words = set(stopwords.words('english'))\n",
        "\n",
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "spacy_stopwords = sp.Defaults.stop_words\n",
        "\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
        "from wordcloud import STOPWORDS as wordcloud_stopwords\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# # for plotting images & adjusting colors\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# libs for data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Don't print warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### In this project, we will create word clouds of journal entries\n",
        "\n",
        "#### Data fields and explanation\n",
        "<ul>\n",
        "  <li><code>Date_time</code></li>\n",
        "  <li><code>days_since_last_entry</code> -- Number of days since the last entry. (Not including the days of both entry)</li>\n",
        "  <li><code>word_count</code> -- word count of the entry</li>\n",
        "  <li><code>TB_senti</code> -- sentiment score from textblob</li>\n",
        "  <li><code>vader_senti</code> -- compound sentiment score from Vader</li>\n",
        "  <li><code>tags</code> -- Tags of the entry (list)</li>\n",
        "  <li><code>journal</code> -- text of the journal</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# read the dataframe\n",
        "df = pd.read_excel('journal_xlsx.xlsx')\n",
        "\n",
        "# set index to date time \n",
        "df.index = df['date_time']\n",
        "\n",
        "# create new columns\n",
        "df['year'] = df['date_time'].dt.year\n",
        "df['month'] = df['date_time'].dt.month\n",
        "df['day'] = df['date_time'].dt.day\n",
        "df['time'] = df['date_time'].dt.time\n",
        "\n",
        "# move columns' positions\n",
        "year = df.pop('year');      df.insert(1, 'year', year, False)\n",
        "month = df.pop('month');    df.insert(2, 'month', month, False)\n",
        "day = df.pop('day');        df.insert(3, 'day', day, False)\n",
        "time = df.pop('time');      df.insert(4, 'time', time, False)\n",
        "\n",
        "# drop date_time col\n",
        "#df.drop(columns=['date_time'], inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <span style='color:LightGreen'> **Create word clouds of common words**</span>\n",
        "\n",
        "#### **Preprocessing process:**  \n",
        "-Pass text data into a tokenizer  \n",
        "-Convert all words to lower  \n",
        "-Remove stop words  \n",
        "-Lemmatize words\n",
        "\n",
        "##### 1) Tokenise and lemmatize words. Resources & refs: \n",
        "<code>https://medium.com/mlearning-ai/nlp-tokenization-stemming-lemmatization-and-part-of-speech-tagging-9088ac068768</code>  \n",
        "<code>https://www.section.io/engineering-education/word-cloud/</code>  \n",
        "\n",
        "##### 2) Lemmatizing words. Resources & refs:\n",
        "<code>https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/</code>  \n",
        "\n",
        "##### 3) Generating word clouds resource. Resources & refs:\n",
        "<code>https://towardsdatascience.com/generate-meaningful-word-clouds-in-python-5b85f5668eeb</code>  \n",
        "<code>https://www.codecademy.com/article/creating-a-word-cloud-with-python</code>  \n",
        "\n",
        "##### Other useful libs to try:  \n",
        "<code>https://pypi.org/project/clevercloud/#description</code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# combine stopwords\n",
        "def all_stop_words():\n",
        "  # read json file and get list of custom stopwords\n",
        "  def custom_stop_words():\n",
        "    with open('extra_stop_words.json', 'r', encoding=\"utf-8\") as file_handler:\n",
        "      data = json.load(file_handler)\n",
        "      #print(json.dumps(data, indent=4, ensure_ascii=False))\n",
        "      return data\n",
        "  \n",
        "  custom_stop_words_set = set(custom_stop_words())\n",
        "  combine_all = nltk_stop_words | spacy_stopwords | sklearn_stopwords | wordcloud_stopwords | custom_stop_words_set\n",
        "  return combine_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lemmatiser 1: simple lemmatise function\n",
        "\n",
        "# INPUT: text input |  OUTPUT: lemmatised string\n",
        "\n",
        "def lemmatiser_simple(txt_input):\n",
        "  # Tokenizing using Textblob\n",
        "  word_tokens = TextBlob(txt_input).words\n",
        "  \n",
        "  # get stopwords\n",
        "  stopwords = all_stop_words()\n",
        "  \n",
        "  # Convert word to lowercase. Add to filtered list if the word is NOT a stopword (check your own custom dictionary as well) \n",
        "  filtered_tokens = [word.lower() for word in word_tokens if (word.lower() not in stopwords)]\n",
        "  \n",
        "  # lemmatize and append to new list if all character in word is alphabetical. Then join all words into a string\n",
        "  lemmatized_string_txt = ' '.join([nltk_lemmatizer.lemmatize(word) for word in filtered_tokens if word.isalpha()])\n",
        "  \n",
        "  return lemmatized_string_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lemmatiser 2: Lemmatise with POS (parts of speech) tag\n",
        "\n",
        "# INPUT: text input |  OUTPUT: lemmatised string\n",
        "def lemmatiser_POS(txt_input):\n",
        "  # Define function to lemmatize each word with its POS tag\n",
        "  def pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "      return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "      return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "      return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "      return wordnet.ADV\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  # get stopwords\n",
        "  stopwords = all_stop_words()\n",
        "\n",
        "  # tokenize the sentence and find the POS tag for each token\n",
        "  pos_tagged = nltk.pos_tag(nltk.word_tokenize(txt_input)) \n",
        "  \n",
        "  # we use our 'pos_tagger' function to make things simpler to understand.\n",
        "  wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
        "  \n",
        "  # init list to hold words lemmatised with tag\n",
        "  lemmatized_tokens = []\n",
        "\n",
        "  for word, tag in wordnet_tagged:\n",
        "    if tag is None:\n",
        "      # if there is no available tag, convert the word to lowercase and hold it.\n",
        "      lem_word = nltk_lemmatizer.lemmatize(word).lower()\n",
        "    else:       \n",
        "      # else use the tag to lemmatize the token. Then convert to lowercase\n",
        "      lem_word = nltk_lemmatizer.lemmatize(word, tag).lower()\n",
        "\n",
        "    # Check if word is all alphabetical & not in stopwords library. If yes, append it to lemmatised tokens\n",
        "    if lem_word.isalpha() and (lem_word not in stopwords):\n",
        "      lemmatized_tokens.append(lem_word)\n",
        "\n",
        "  # join sentence into a string & return\n",
        "  return \" \".join(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing function. \n",
        "# 1) if not a stop word\n",
        "# 2) if more than 1\n",
        "# 3) convert to lower case\n",
        "# 4) lemmatize the word\n",
        "\n",
        "def preprocess_text_get_dict_word_freq(txt_input, lemmatiser='simple', k_num_of_words=5000):\n",
        "  # get word frequency dictionary\n",
        "  def get_word_freq_dict(txt_input):\n",
        "    freq_dict = {}\n",
        "    for item in txt_input.split():\n",
        "      if (item in freq_dict):\n",
        "        freq_dict[item] += 1\n",
        "      else:\n",
        "        freq_dict[item] = 1\n",
        "    return freq_dict\n",
        "  \n",
        "  if lemmatiser == 'simple':\n",
        "    lemmatized_string_txt = lemmatiser_simple(txt_input)\n",
        "  elif lemmatiser == 'pos':\n",
        "    lemmatized_string_txt = lemmatiser_POS(txt_input)\n",
        "\n",
        "  # create a dictionary of word frequencies\n",
        "  word_freq = get_word_freq_dict(lemmatized_string_txt)\n",
        "  \n",
        "  # sort the dictionary\n",
        "  word_freq = {k: v for k, v in sorted(word_freq.items(), reverse=True, key=lambda item: item[1])}\n",
        "\n",
        "  # Get first K items in dictionary\n",
        "  dict_with_k_limit = dict(list(word_freq.items())[0: k_num_of_words])\n",
        "\n",
        "  return dict_with_k_limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate word cloud figure function\n",
        "'''\n",
        "params:\n",
        "1) input: list of texts | 1 single line of text\n",
        "2) cloud_bg: None (default rectangle bg) | img_path\n",
        "3) size: None (default=[2048, 1080]) | [width(px), height(px)] | 'use img size'\n",
        "4) save_img: None | '(file_name)'.jpg\n",
        "''' \n",
        "\n",
        "# other nice sizes = [2048, 1080], [800, 400]\n",
        "\n",
        "def generate_word_cloud(word_freq_dict, img_path=None, img_size=[800, 400]):\n",
        "  \n",
        "  # if there is an image path input\n",
        "  if img_path is not None:\n",
        "    img = Image.open(img_path)\n",
        "    background_image = np.array(img)\n",
        "    # create word cloud obj (use bg image, and use original img width and height)\n",
        "    word_cloud_obj = WordCloud(min_word_length = 2,\n",
        "                            background_color = 'black', \n",
        "                            mask = background_image, \n",
        "                            width=img_size[0], height=img_size[1])\n",
        "                            #width=img.width, height=img.height)\n",
        "\n",
        "  # if no img path input\n",
        "  else:\n",
        "    # create word cloud obj (use bg image, and use original img width and height)\n",
        "    word_cloud_obj = WordCloud(min_word_length = 2,\n",
        "                              background_color = 'black',\n",
        "                              width=img_size[0], height=img_size[1])\n",
        "  \n",
        "  # generate the word cloud from word frequencies\n",
        "  figure = word_cloud_obj.generate_from_frequencies(word_freq_dict)\n",
        "  \n",
        "  # draw to figure\n",
        "  plt.imshow(word_cloud_obj, interpolation='bilinear')\n",
        "  plt.axis(('off'))\n",
        "  \n",
        "  return figure "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get random img\n",
        "import os\n",
        "from random import randint\n",
        "\n",
        "def get_cloud_img(img_name='random'):\n",
        "  # Set variables\n",
        "  curr_py_path = os.getcwd()\n",
        "  img_folder_name = '\\\\word_cloud_pics'\n",
        "\n",
        "  # get list of images\n",
        "  list_img_names = os.listdir(curr_py_path + img_folder_name)\n",
        "\n",
        "  if img_name == 'random':\n",
        "    # get a random image\n",
        "    img_name = list_img_names[randint(0, len(list_img_names))]\n",
        "\n",
        "  # get file path of img and return\n",
        "  return curr_py_path + img_folder_name + '\\\\' + img_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select multiple different entries and concat to 1 string\n",
        "text = df['journal'].str.cat(sep=', \\n')\n",
        "\n",
        "# text = df.iloc[300:700]['journal'].str.cat(sep=', \\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = 'cloud.jpg'\n",
        "path = get_cloud_img(img)\n",
        "print(path)\n",
        "\n",
        "word_freq = preprocess_text_get_dict_word_freq(text, lemmatiser='pos')\n",
        "\n",
        "cloud_fig = generate_word_cloud(word_freq_dict=word_freq, img_path=path, img_size=[500, 500]) #[2048, 1080]\n",
        "plt.show()\n",
        "\n",
        "# save\n",
        "cloud_fig.to_file('smth.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
